# ğŸ™ï¸ Dia Narration Model
### *Advanced Text-to-Speech with Multi-Speaker Dialogue*

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-Latest-red.svg)](https://pytorch.org)
[![Gradio](https://img.shields.io/badge/Gradio-Interactive-orange.svg)](https://gradio.app)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸŒŸ Overview

**Dia Narration Model** is a cutting-edge text-to-speech synthesis tool built on Nari Labs' powerful **Dia TTS model** â€” a 1.6B parameter open-source neural network that generates incredibly realistic multi-speaker dialogue. Transform your scripts into natural-sounding audio with advanced voice conditioning and seamless speaker transitions.

### âœ¨ What Makes It Special?

- ğŸ­ **Multi-Speaker Support** â€” Natural dialogue with distinct voices
- ğŸ¨ **Voice Cloning** â€” Condition on reference audio for custom voices  
- ğŸ§  **Smart Chunking** â€” Intelligent text processing for long narrations
- ğŸ”— **Seamless Audio Chaining** â€” Continuous output without breaks
- ğŸ›ï¸ **Fine-Tuned Control** â€” Adjustable parameters for perfect results

---

## ğŸš€ Key Features

### ğŸ”¤ **Advanced Tokenization**
- Intelligent sentence splitting using NLTK
- GPT-2 tokenizer for precise chunk sizing
- Optimized processing for any text length

### ğŸ“Š **Dynamic Chunking System**
- Adaptive ~64 token chunks for optimal processing
- Smart handling of short texts (â‰¤80 tokens)
- Prevents over-splitting while maintaining quality

### ğŸµ **Seamless Audio Chaining**
- Smooth transitions between audio segments
- 2.5-second overlap for natural continuity
- No awkward pauses or breaks

### ğŸ¤ **Voice Conditioning & Cloning**
- Upload reference audio for voice matching
- Emotion and tone preservation
- Advanced conditioning capabilities

### ğŸ–¥ï¸ **Interactive Gradio Interface**
- User-friendly web interface
- Real-time parameter adjustment
- Professional audio controls

---

## ğŸ“‹ Prerequisites

Before getting started, ensure you have:

- **Python 3.10+** installed
- **Git** for repository cloning
- **Audio drivers** for playback
- At least **4GB RAM** recommended

---

## âš¡ Quick Start

### 1ï¸âƒ£ **Clone Repository**
```bash
git clone https://github.com/Hanyaa-Technologies/Dia-by-nari-labs-Narration-model.git
cd Dia-by-nari-labs-Narration-model
```

### 2ï¸âƒ£ **Environment Setup**

Choose your preferred method:

#### ğŸ **Option A: Using venv + pip**
```bash
# Create virtual environment
python -m venv venv

# Activate environment
source venv/bin/activate        # ğŸ§ Linux/macOS
# OR
venv\Scripts\activate           # ğŸªŸ Windows

# Install dependencies
pip install -r requirements.txt
```

#### ğŸ **Option B: Using conda**
```bash
# Create conda environment
conda create -n dia-tts python=3.10 -y

# Activate environment
conda activate dia-tts

# Install dependencies
pip install -r requirements.txt
```

### 3ï¸âƒ£ **Launch Application**
```bash
python experiment4.py
```
Video link - https://1drv.ms/v/c/0fd555f519abe747/ET5IMBLdwqREjsFnXe-PBnQBc9IqTXnaH2qsZxcpzvSjNg?e=SQyRQN

ğŸ‰ **That's it!** Your Gradio interface will open automatically in your default browser.

---

## ğŸ“ Project Structure

```
ğŸ“¦ Dia-Narration-Model/
â”œâ”€â”€ ğŸ™ï¸ dia/                    # Core TTS modules
â”œâ”€â”€ ğŸ³ docker/                 # Docker configuration
â”œâ”€â”€ ğŸ“ example/                # Sample files & demos
â”œâ”€â”€ âš™ï¸ cli.py                  # Command line interface
â”œâ”€â”€ ğŸµ example_prompt.mp3      # Sample audio prompt
â”œâ”€â”€ ğŸ§ª experiment4.py          # Main Gradio application
â”œâ”€â”€ ğŸ“Š test.py                 # Testing utilities
â”œâ”€â”€ ğŸ–¼ï¸ horizontal.png          # Project banner
â”œâ”€â”€ ğŸ“‹ requirements.txt        # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md              # This file
â”œâ”€â”€ âš–ï¸ LICENSE                # License information
â””â”€â”€ ğŸ”’ pyproject.toml         # Project configuration
```

---

## ğŸ¯ Usage Guide

### ğŸ–¥ï¸ **Web Interface**

1. **Launch the app**: Run `python experiment4.py`
2. **Enter your script**: Use `[S1]` and `[S2]` tags for different speakers
3. **Upload reference audio** (optional): For voice cloning
4. **Adjust parameters**: Fine-tune CFG scale, temperature, etc.
5. **Generate**: Click the magic button and listen!

### ğŸ“ **Script Format Example**

```
[S1] Welcome to our podcast! I'm excited to discuss today's topic.
[S2] Thanks for having me! This is going to be a great conversation.
[S1] Let's dive right in. What are your thoughts on...
```

### ğŸšï¸ **Parameter Controls**

| Parameter | Description | Recommended Range |
|-----------|-------------|-------------------|
| **CFG Scale** | Controls adherence to prompt | 1.0 - 3.0 |
| **Temperature** | Output randomness | 0.7 - 1.2 |
| **Top-p** | Nucleus sampling threshold | 0.8 - 0.95 |
| **Seed** | Reproducibility control | Any integer |

---

## ğŸ› ï¸ Advanced Configuration

### ğŸµ **Audio Settings**
- **Sample Rate**: 24kHz (default)
- **Bit Depth**: 16-bit
- **Format**: WAV/MP3 compatible
- **Chunk Size**: Automatically optimized

### ğŸ’¾ **Memory Optimization**
- Automatic GPU/CPU detection
- Efficient batch processing  
- Memory-mapped audio loading
- Smart caching system

---

## ğŸ”§ Troubleshooting

### Common Issues & Solutions

#### ğŸš¨ **"ModuleNotFoundError: No module named 'xxx'"**
```bash
pip install -r requirements.txt --upgrade
```

#### ğŸš¨ **NLTK Data Missing**
```python
import nltk
nltk.download('punkt')
```

#### ğŸš¨ **CUDA Out of Memory**
- Reduce chunk size in settings
- Close other GPU-intensive applications
- Try CPU-only mode

#### ğŸš¨ **Audio Quality Issues**
- Check sample rate compatibility
- Verify reference audio quality
- Adjust CFG scale parameter

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. ğŸ´ **Fork** the repository
2. ğŸŒŸ **Create** a feature branch
3. ğŸ’» **Commit** your changes
4. ğŸ“¤ **Push** to your branch
5. ğŸ”„ **Create** a Pull Request

### ğŸ“‹ **Contribution Guidelines**
- Follow PEP 8 coding standards
- Add tests for new features
- Update documentation
- Be respectful and collaborative

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Nari Labs** for the incredible Dia TTS model
- **Hugging Face** for the Transformers library
- **Gradio** team for the amazing interface framework
- **PyTorch** community for the deep learning foundation

<div align="center">

### ğŸŒŸ **Star this project if you found it helpful!** â­

**Made with â¤ï¸ by [Hanyaa Technologies](https://github.com/Hanyaa-Technologies)**

</div>
